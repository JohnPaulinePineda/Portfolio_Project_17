---
title: 'Feature Selection : Selecting Informative Predictors Using Recursive Feature Elimination'
author: "<b><a href='https://github.com/JohnPaulinePineda'>John Pauline Pineda</a></b>"
date: "December 19, 2022"
output: 
  html_document:
    toc: true
    toc_depth: 3
    theme: readable
    highlight: tango
    css: doc.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=15, fig.height=10)
```
# **1. Table of Contents**
|
| This project implements **Recursive Feature Elimination** in selecting informative predictors for a modelling problem using the **Random Forest**, **Linear Discriminant Analysis**, **Naive Bayes**, **Logistic Regression**, **Support Vector Machine** and **K-Nearest Neighbors** model structures in <mark style="background-color: #CCECFF">**R**</mark>. The resulting predictions derived from the candidate models applying **Recursive Feature Elimination** were evaluated in terms of their discrimination power using the area under the receiver operating characteristics curve (AUROC) metric. The AUROC values were compared to those of the baseline models which made use of the full data without any form of feature selection, or implemented a model-specific feature selection process. All results were consolidated in a [<span style="color: #FF0000">**Summary**</span>](#summary) presented at the end of the document.
|
| Feature selection is the process of reducing the number of input variables by eliminating redundant or unimportant features and narrowing down the set of features to those most relevant to the machine learning approach, resulting to simpler explainable models, shorter training times due to a more precise subset of features, reduction of variance, increase in precision estimates and more robust solution against the curse of high dimensionality. The algorithms applied in this study (mostly contained in the <mark style="background-color: #CCECFF">**caret**</mark> package) are wrapper feature selection methods which attempt to consider the task of selecting feature subsets as a search problem, whereby their quality is assessed with the preparation, evaluation, and comparison between different feature combinations.
|
##  1.1 Sample Data
|
| The <mark style="background-color: #EEEEEE;color: #FF0000">**AlzheimerDisease**</mark>  dataset from the  <mark style="background-color: #CCECFF">**AppliedPredictiveModeling**</mark> package was used for this illustrated example.
|
| Preliminary dataset assessment:
|
| **[A]** 333 rows (observations)
|      **[A.1]** Train Set = 267 observations
|      **[A.2]** Test Set = 66 observations
| 
| **[B]** 128 columns (variables)
|      **[B.1]** 1/128 response = <span style="color: #FF0000">Class</span> variable (factor)
|             **[B.1.1]** Levels = <span style="color: #FF0000">Class=Control</span> < <span style="color: #FF0000">Class=Impaired</span>
|      **[B.2]** 127/128 predictors = All remaining variables (3/127 factor + 124/127 numeric)
|     
| 

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.1, warning=FALSE, message=FALSE}
##################################
# Loading R libraries
##################################
library(AppliedPredictiveModeling)
library(caret)
library(rpart)
library(lattice)
library(dplyr)
library(tidyr)
library(moments)
library(skimr)
library(RANN)
library(pls)
library(corrplot)
library(tidyverse)
library(lares)
library(DMwR2)
library(gridExtra)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(stats)
library(nnet)
library(elasticnet)
library(earth)
library(party)
library(kernlab)
library(randomForest)
library(Cubist)
library(pROC)
library(mda)
library(klaR)
library(pamr)

##################################
# Loading source and
# formulating the train set
##################################
data(AlzheimerDisease)
Alzheimer <- predictors
Alzheimer$Class <- diagnosis

##################################
# Decomposing the Genotype factor
# into binary dummy variables
##################################

## Decompose the genotype factor into binary dummy variables

Alzheimer$E2 <- Alzheimer$E3 <- Alzheimer$E4 <- 0
Alzheimer$E2[grepl("2", Alzheimer$Genotype)] <- 1
Alzheimer$E3[grepl("3", Alzheimer$Genotype)] <- 1
Alzheimer$E4[grepl("4", Alzheimer$Genotype)] <- 1
Alzheimer_Original <- Alzheimer

##################################
# Removing baseline predictors
##################################
Alzheimer <- Alzheimer[,!(names(Alzheimer) %in% c("Genotype", "age", "tau", "p_tau", "Ab_42", "male"))]

##################################
# Partitoning the data into
# train and test sets
##################################
set.seed(12345678)
Alzheimer_Train_Index <- createDataPartition(Alzheimer$Class,p=0.8)[[1]]
Alzheimer_Train <- Alzheimer[ Alzheimer_Train_Index, ]
Alzheimer_Test  <- Alzheimer[-Alzheimer_Train_Index, ]

##################################
# Performing a general exploration of the train set
##################################
dim(Alzheimer_Train)
str(Alzheimer_Train)
summary(Alzheimer_Train)

##################################
# Performing a general exploration of the test set
##################################
dim(Alzheimer_Test)
str(Alzheimer_Test)
summary(Alzheimer_Test)

##################################
# Formulating a data type assessment summary
##################################
PDA <- Alzheimer_Train
(PDA.Summary <- data.frame(
  Column.Index=c(1:length(names(PDA))),
  Column.Name= names(PDA), 
  Column.Type=sapply(PDA, function(x) class(x)), 
  row.names=NULL)
)
```

</details>

##  1.2 Data Quality Assessment
|
| **[A]** No missing observations noted for any variable.
|
| **[B]** Low variance observed for 2 variables with First.Second.Mode.Ratio>5.
|      **[B.1]** <span style="color: #FF0000">E2</span> variable (factor)
|      **[B.2]** <span style="color: #FF0000">E3</span> variable (factor)
|
| **[C]** No low variance observed for any variable with Unique.Count.Ratio<0.01.
|
| **[D]** No high skewness observed for any variable with Skewness>3 or Skewness<(-3).
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.2, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DQA <- Alzheimer_Train

##################################
# Formulating an overall data quality assessment summary
##################################
(DQA.Summary <- data.frame(
  Column.Index=c(1:length(names(DQA))),
  Column.Name= names(DQA), 
  Column.Type=sapply(DQA, function(x) class(x)), 
  Row.Count=sapply(DQA, function(x) nrow(DQA)),
  NA.Count=sapply(DQA,function(x)sum(is.na(x))),
  Fill.Rate=sapply(DQA,function(x)format(round((sum(!is.na(x))/nrow(DQA)),3),nsmall=3)),
  row.names=NULL)
)

##################################
# Listing all predictors
##################################
DQA.Predictors <- DQA[,!names(DQA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DQA.Predictors.Numeric <- DQA.Predictors[,!names(DQA.Predictors) %in% c("E2","E3","E4")]
DQA.Predictors.Numeric <- as.data.frame(sapply(DQA.Predictors.Numeric,function(x) as.numeric(x)))

if (length(names(DQA.Predictors.Numeric))>0) {
    print(paste0("There are ",
               (length(names(DQA.Predictors.Numeric))),
               " numeric predictor variable(s)."))
} else {
  print("There are no numeric predictor variables.")
}

##################################
# Listing all factor predictors
##################################
DQA.Predictors.Factor <- DQA.Predictors[,names(DQA.Predictors) %in% c("E2","E3","E4")]
DQA.Predictors.Factor <- as.data.frame(sapply(DQA.Predictors.Factor,function(x) as.factor(x)))

if (length(names(DQA.Predictors.Factor))>0) {
    print(paste0("There are ",
               (length(names(DQA.Predictors.Factor))),
               " factor predictor variable(s)."))
} else {
  print("There are no factor predictor variables.")
}

##################################
# Formulating a data quality assessment summary for factor predictors
##################################
if (length(names(DQA.Predictors.Factor))>0) {
  
  ##################################
  # Formulating a function to determine the first mode
  ##################################
  FirstModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    ux[tab == max(tab)]
  }

  ##################################
  # Formulating a function to determine the second mode
  ##################################
  SecondModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    fm = ux[tab == max(tab)]
    sm = x[!(x %in% fm)]
    usm <- unique(sm)
    tabsm <- tabulate(match(sm, usm))
    ifelse(is.na(usm[tabsm == max(tabsm)])==TRUE,
           return("x"),
           return(usm[tabsm == max(tabsm)]))
  }
  
  (DQA.Predictors.Factor.Summary <- data.frame(
  Column.Name= names(DQA.Predictors.Factor), 
  Column.Type=sapply(DQA.Predictors.Factor, function(x) class(x)), 
  Unique.Count=sapply(DQA.Predictors.Factor, function(x) length(unique(x))),
  First.Mode.Value=sapply(DQA.Predictors.Factor, function(x) as.character(FirstModes(x)[1])),
  Second.Mode.Value=sapply(DQA.Predictors.Factor, function(x) as.character(SecondModes(x)[1])),
  First.Mode.Count=sapply(DQA.Predictors.Factor, function(x) sum(na.omit(x) == FirstModes(x)[1])),
  Second.Mode.Count=sapply(DQA.Predictors.Factor, function(x) sum(na.omit(x) == SecondModes(x)[1])),
  Unique.Count.Ratio=sapply(DQA.Predictors.Factor, function(x) format(round((length(unique(x))/nrow(DQA.Predictors.Factor)),3), nsmall=3)),
  First.Second.Mode.Ratio=sapply(DQA.Predictors.Factor, function(x) format(round((sum(na.omit(x) == FirstModes(x)[1])/sum(na.omit(x) == SecondModes(x)[1])),3), nsmall=3)),
  row.names=NULL)
  )
  
} 

##################################
# Formulating a data quality assessment summary for numeric predictors
##################################
if (length(names(DQA.Predictors.Numeric))>0) {
  
  ##################################
  # Formulating a function to determine the first mode
  ##################################
  FirstModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    ux[tab == max(tab)]
  }

  ##################################
  # Formulating a function to determine the second mode
  ##################################
  SecondModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    fm = ux[tab == max(tab)]
    sm = na.omit(x)[!(na.omit(x) %in% fm)]
    usm <- unique(sm)
    tabsm <- tabulate(match(sm, usm))
    ifelse(is.na(usm[tabsm == max(tabsm)])==TRUE,
           return(0.00001),
           return(usm[tabsm == max(tabsm)]))
  }
  
  (DQA.Predictors.Numeric.Summary <- data.frame(
  Column.Name= names(DQA.Predictors.Numeric), 
  Column.Type=sapply(DQA.Predictors.Numeric, function(x) class(x)), 
  Unique.Count=sapply(DQA.Predictors.Numeric, function(x) length(unique(x))),
  Unique.Count.Ratio=sapply(DQA.Predictors.Numeric, function(x) format(round((length(unique(x))/nrow(DQA.Predictors.Numeric)),3), nsmall=3)),
  First.Mode.Value=sapply(DQA.Predictors.Numeric, function(x) format(round((FirstModes(x)[1]),3),nsmall=3)),
  Second.Mode.Value=sapply(DQA.Predictors.Numeric, function(x) format(round((SecondModes(x)[1]),3),nsmall=3)),
  First.Mode.Count=sapply(DQA.Predictors.Numeric, function(x) sum(na.omit(x) == FirstModes(x)[1])),
  Second.Mode.Count=sapply(DQA.Predictors.Numeric, function(x) sum(na.omit(x) == SecondModes(x)[1])),
  First.Second.Mode.Ratio=sapply(DQA.Predictors.Numeric, function(x) format(round((sum(na.omit(x) == FirstModes(x)[1])/sum(na.omit(x) == SecondModes(x)[1])),3), nsmall=3)),
  Minimum=sapply(DQA.Predictors.Numeric, function(x) format(round(min(x,na.rm = TRUE),3), nsmall=3)),
  Mean=sapply(DQA.Predictors.Numeric, function(x) format(round(mean(x,na.rm = TRUE),3), nsmall=3)),
  Median=sapply(DQA.Predictors.Numeric, function(x) format(round(median(x,na.rm = TRUE),3), nsmall=3)),
  Maximum=sapply(DQA.Predictors.Numeric, function(x) format(round(max(x,na.rm = TRUE),3), nsmall=3)),
  Skewness=sapply(DQA.Predictors.Numeric, function(x) format(round(skewness(x,na.rm = TRUE),3), nsmall=3)),
  Kurtosis=sapply(DQA.Predictors.Numeric, function(x) format(round(kurtosis(x,na.rm = TRUE),3), nsmall=3)),
  Percentile25th=sapply(DQA.Predictors.Numeric, function(x) format(round(quantile(x,probs=0.25,na.rm = TRUE),3), nsmall=3)),
  Percentile75th=sapply(DQA.Predictors.Numeric, function(x) format(round(quantile(x,probs=0.75,na.rm = TRUE),3), nsmall=3)),
  row.names=NULL)
  )  
  
}

##################################
# Identifying potential data quality issues
##################################

##################################
# Checking for missing observations
##################################
if ((nrow(DQA.Summary[DQA.Summary$NA.Count>0,]))>0){
  print(paste0("Missing observations noted for ",
               (nrow(DQA.Summary[DQA.Summary$NA.Count>0,])),
               " variable(s) with NA.Count>0 and Fill.Rate<1.0."))
  DQA.Summary[DQA.Summary$NA.Count>0,]
} else {
  print("No missing observations noted.")
}

##################################
# Checking for zero or near-zero variance predictors
##################################
if (length(names(DQA.Predictors.Factor))==0) {
  print("No factor predictors noted.")
} else if (nrow(DQA.Predictors.Factor.Summary[as.numeric(as.character(DQA.Predictors.Factor.Summary$First.Second.Mode.Ratio))>5,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Predictors.Factor.Summary[as.numeric(as.character(DQA.Predictors.Factor.Summary$First.Second.Mode.Ratio))>5,])),
               " factor variable(s) with First.Second.Mode.Ratio>5."))
  DQA.Predictors.Factor.Summary[as.numeric(as.character(DQA.Predictors.Factor.Summary$First.Second.Mode.Ratio))>5,]
} else {
  print("No low variance factor predictors due to high first-second mode ratio noted.")
}

if (length(names(DQA.Predictors.Numeric))==0) {
  print("No numeric predictors noted.")
} else if (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$First.Second.Mode.Ratio))>5,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$First.Second.Mode.Ratio))>5,])),
               " numeric variable(s) with First.Second.Mode.Ratio>5."))
  DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$First.Second.Mode.Ratio))>5,]
} else {
  print("No low variance numeric predictors due to high first-second mode ratio noted.")
}

if (length(names(DQA.Predictors.Numeric))==0) {
  print("No numeric predictors noted.")
} else if (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Unique.Count.Ratio))<0.01,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Unique.Count.Ratio))<0.01,])),
               " numeric variable(s) with Unique.Count.Ratio<0.01."))
  DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Unique.Count.Ratio))<0.01,]
} else {
  print("No low variance numeric predictors due to low unique count ratio noted.")
}

##################################
# Checking for skewed predictors
##################################
if (length(names(DQA.Predictors.Numeric))==0) {
  print("No numeric predictors noted.")
} else if (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))>3 |
                                               as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))<(-3),])>0){
  print(paste0("High skewness observed for ",
  (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))>3 |
                                               as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))<(-3),])),
  " numeric variable(s) with Skewness>3 or Skewness<(-3)."))
  DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))>3 |
                                 as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))<(-3),]
} else {
  print("No skewed numeric predictors noted.")
}

```

</details>

##  1.3 Data Preprocessing

###  1.3.1 Outlier
|
| **[A]** Few outliers noted for most variables  with the numeric data visualized through a boxplot including observations classified as suspected outliers using the IQR criterion. The IQR criterion means that all observations above the (75th percentile + 1.5 x IQR) or below the (25th percentile - 1.5 x IQR) are suspected outliers, where IQR is the difference between the third quartile (75th percentile) and first quartile (25th percentile). Outlier treatment for numerical stability remains optional depending on potential model requirements for the subsequent steps. 6 variables were observed with at least 10 outliers listed as follows:
|      **[A.1]** <span style="color: #FF0000">Apolipoprotein_CI</span> variable (10 outliers detected)
|      **[A.2]** <span style="color: #FF0000">Cortisol</span> variable (11 outliers detected)
|      **[A.3]** <span style="color: #FF0000">IL_17E</span> variable (11 outliers detected)
|      **[A.4]** <span style="color: #FF0000">IL6</span> variable (19 outliers detected)
|      **[A.5]** <span style="color: #FF0000">MCP_2</span> variable (21 outliers detected)
|      **[A.6]** <span style="color: #FF0000">Prostatic_Acid_Phospatase</span> variable (10 outliers detected)
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.1, warning=FALSE, message=FALSE, fig.width=15, fig.height=3}
##################################
# Loading dataset
##################################
DPA <- Alzheimer_Train

##################################
# Listing all predictors
##################################
DPA.Predictors <- DPA[,!names(DPA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DPA.Predictors.Numeric <- DPA.Predictors[,!names(DPA.Predictors) %in% c("E2","E3","E4")]
DPA.Predictors.Numeric <- as.data.frame(sapply(DPA.Predictors.Numeric,function(x) as.numeric(x)))

##################################
# Identifying outliers for the numeric predictors
##################################
OutlierCountList <- c()

for (i in 1:ncol(DPA.Predictors.Numeric)) {
  Outliers <- boxplot.stats(DPA.Predictors.Numeric[,i])$out
  OutlierCount <- length(Outliers)
  OutlierCountList <- append(OutlierCountList,OutlierCount)
  OutlierIndices <- which(DPA.Predictors.Numeric[,i] %in% c(Outliers))
  boxplot(DPA.Predictors.Numeric[,i], 
          ylab = names(DPA.Predictors.Numeric)[i], 
          main = names(DPA.Predictors.Numeric)[i],
          horizontal=TRUE)
  mtext(paste0(OutlierCount, " Outlier(s) Detected"))
}

OutlierCountSummary <- as.data.frame(cbind(names(DPA.Predictors.Numeric),(OutlierCountList)))
names(OutlierCountSummary) <- c("NumericPredictors","OutlierCount")
OutlierCountSummary$OutlierCount <- as.numeric(as.character(OutlierCountSummary$OutlierCount))
NumericPredictorWithOutlierCount <- nrow(OutlierCountSummary[OutlierCountSummary$OutlierCount>0,])
print(paste0(NumericPredictorWithOutlierCount, " numeric variable(s) were noted with outlier(s)." ))

##################################
# Gathering descriptive statistics
##################################
(DPA_Skimmed <- skim(DPA.Predictors.Numeric))

###################################
# Verifying the data dimensions
###################################
dim(DPA.Predictors.Numeric)

```

</details>

###  1.3.2 Zero and Near-Zero Variance
|
| **[A]** Low variance noted for 2 variables from the previous data quality assessment using a lower threshold.
|
| **[B]** No low variance noted for any variable using a preprocessing summary from the <mark style="background-color: #CCECFF">**caret**</mark> package. The <span style="color: #0000FF">nearZeroVar</span> method using both the <span style="color: #0000FF">freqCut</span> and <span style="color: #0000FF">uniqueCut</span> criteria set at 95/5 and 10, respectively, were applied on the dataset.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.2, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- DPA.Predictors

##################################
# Gathering descriptive statistics
##################################
(DPA_Skimmed <- skim(DPA))

##################################
# Identifying columns with low variance
###################################
DPA_LowVariance <- nearZeroVar(DPA,
                               freqCut = 95/5,
                               uniqueCut = 10,
                               saveMetrics= TRUE)
(DPA_LowVariance[DPA_LowVariance$nzv,])

if ((nrow(DPA_LowVariance[DPA_LowVariance$nzv,]))==0){
  
  print("No low variance predictors noted.")
  
} else {

  print(paste0("Low variance observed for ",
               (nrow(DPA_LowVariance[DPA_LowVariance$nzv,])),
               " numeric variable(s) with First.Second.Mode.Ratio>4 and Unique.Count.Ratio<0.10."))
  
  DPA_LowVarianceForRemoval <- (nrow(DPA_LowVariance[DPA_LowVariance$nzv,]))
  
  print(paste0("Low variance can be resolved by removing ",
               (nrow(DPA_LowVariance[DPA_LowVariance$nzv,])),
               " numeric variable(s)."))
  
  for (j in 1:DPA_LowVarianceForRemoval) {
  DPA_LowVarianceRemovedVariable <- rownames(DPA_LowVariance[DPA_LowVariance$nzv,])[j]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_LowVarianceRemovedVariable))
  }
  
  DPA %>%
  skim() %>%
  dplyr::filter(skim_variable %in% rownames(DPA_LowVariance[DPA_LowVariance$nzv,]))

  ##################################
  # Filtering out columns with low variance
  #################################
  DPA_ExcludedLowVariance <- DPA[,!names(DPA) %in% rownames(DPA_LowVariance[DPA_LowVariance$nzv,])]
  
  ##################################
  # Gathering descriptive statistics
  ##################################
  (DPA_ExcludedLowVariance_Skimmed <- skim(DPA_ExcludedLowVariance))
}

```

</details>

###  1.3.3 Collinearity
|
| **[A]** No high correlation > 95% were noted for any variable pair as confirmed using the preprocessing summaries from the <mark style="background-color: #CCECFF">**caret**</mark> and <mark style="background-color: #CCECFF">**lares**</mark> packages.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.3, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- Alzheimer_Train

##################################
# Listing all predictors
##################################
DPA.Predictors <- DPA[,!names(DPA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DPA.Predictors.Numeric <- DPA.Predictors[,!names(DPA.Predictors) %in% c("E2","E3","E4")]
DPA.Predictors.Numeric <- as.data.frame(sapply(DPA.Predictors.Numeric,function(x) as.numeric(x)))

##################################
# Visualizing pairwise correlation between predictors
##################################
DPA_CorrelationTest <- cor.mtest(DPA.Predictors.Numeric,
                       method = "pearson",
                       conf.level = .95)

corrplot(cor(DPA.Predictors.Numeric,
             method = "pearson",
             use="pairwise.complete.obs"), 
         method = "circle",
         type = "upper", 
         order = "original", 
         tl.col = "black", 
         tl.cex = 0.75,
         tl.srt = 90, 
         sig.level = 0.05, 
         p.mat = DPA_CorrelationTest$p,
         insig = "blank")



##################################
# Identifying the highly correlated variables
##################################
DPA_Correlation <-  cor(DPA.Predictors.Numeric, 
                        method = "pearson",
                        use="pairwise.complete.obs")
(DPA_HighlyCorrelatedCount <- sum(abs(DPA_Correlation[upper.tri(DPA_Correlation)]) > 0.95))

if (DPA_HighlyCorrelatedCount == 0) {
  print("No highly correlated predictors noted.")
} else {
  print(paste0("High correlation observed for ",
               (DPA_HighlyCorrelatedCount),
               " pairs of numeric variable(s) with Correlation.Coefficient>0.95."))
  
  (DPA_HighlyCorrelatedPairs <- corr_cross(DPA.Predictors.Numeric,
  max_pvalue = 0.05, 
  top = DPA_HighlyCorrelatedCount,
  rm.na = TRUE,
  grid = FALSE
))
  
}


if (DPA_HighlyCorrelatedCount > 0) {
  DPA_HighlyCorrelated <- findCorrelation(DPA_Correlation, cutoff = 0.95)
  
  (DPA_HighlyCorrelatedForRemoval <- length(DPA_HighlyCorrelated))
  
  print(paste0("High correlation can be resolved by removing ",
               (DPA_HighlyCorrelatedForRemoval),
               " numeric variable(s)."))
  
  for (j in 1:DPA_HighlyCorrelatedForRemoval) {
  DPA_HighlyCorrelatedRemovedVariable <- colnames(DPA.Predictors.Numeric)[DPA_HighlyCorrelated[j]]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_HighlyCorrelatedRemovedVariable))
  }
  
  ##################################
  # Filtering out columns with high correlation
  #################################
  DPA_ExcludedHighCorrelation <- DPA[,-DPA_HighlyCorrelated]
  
  ##################################
  # Gathering descriptive statistics
  ##################################
  (DPA_ExcludedHighCorrelation_Skimmed <- skim(DPA_ExcludedHighCorrelation))

}

```

</details>

###  1.3.4 Linear Dependencies
|
| **[A]** No linear dependencies noted for any subset of variables using the preprocessing summary from the <mark style="background-color: #CCECFF">**caret**</mark> package applying the <span style="color: #0000FF">findLinearCombos</span> method which utilizes the QR decomposition of a matrix to enumerate sets of linear combinations (if they exist). 
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.4, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- Alzheimer_Train

##################################
# Listing all predictors
##################################
DPA.Predictors <- DPA[,!names(DPA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DPA.Predictors.Numeric <- DPA.Predictors[,!names(DPA.Predictors) %in% c("E2","E3","E4")]
DPA.Predictors.Numeric <- as.data.frame(sapply(DPA.Predictors.Numeric,function(x) as.numeric(x)))

##################################
# Identifying the linearly dependent variables
##################################
DPA_LinearlyDependent <- findLinearCombos(DPA.Predictors.Numeric)

(DPA_LinearlyDependentCount <- length(DPA_LinearlyDependent$linearCombos))

if (DPA_LinearlyDependentCount == 0) {
  print("No linearly dependent predictors noted.")
} else {
  print(paste0("Linear dependency observed for ",
               (DPA_LinearlyDependentCount),
               " subset(s) of numeric variable(s)."))
  
  for (i in 1:DPA_LinearlyDependentCount) {
    DPA_LinearlyDependentSubset <- colnames(DPA.Predictors.Numeric)[DPA_LinearlyDependent$linearCombos[[i]]]
    print(paste0("Linear dependent variable(s) for subset ",
                 i,
                 " include: ",
                 DPA_LinearlyDependentSubset))
  }
  
}

##################################
# Identifying the linearly dependent variables for removal
##################################

if (DPA_LinearlyDependentCount > 0) {
  DPA_LinearlyDependent <- findLinearCombos(DPA.Predictors.Numeric)
  
  DPA_LinearlyDependentForRemoval <- length(DPA_LinearlyDependent$remove)
  
  print(paste0("Linear dependency can be resolved by removing ",
               (DPA_LinearlyDependentForRemoval),
               " numeric variable(s)."))
  
  for (j in 1:DPA_LinearlyDependentForRemoval) {
  DPA_LinearlyDependentRemovedVariable <- colnames(DPA.Predictors.Numeric)[DPA_LinearlyDependent$remove[j]]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_LinearlyDependentRemovedVariable))
  }
  
  ##################################
  # Filtering out columns with linear dependency
  #################################
  DPA_ExcludedLinearlyDependent <- DPA[,-DPA_LinearlyDependent$remove]
  
  ##################################
  # Gathering descriptive statistics
  ##################################
  (DPA_ExcludedLinearlyDependent_Skimmed <- skim(DPA_ExcludedLinearlyDependent))

}

```

</details>

###  1.3.5 Pre-Processed Dataset
|
| **[A]** 333 rows (observations)
|      **[A.1]** Train Set = 267 observations
|      **[A.2]** Test Set = 66 observations
| 
| **[B]** 128 columns (variables)
|      **[B.1]** 1/128 response = <span style="color: #FF0000">Class</span> variable (factor)
|             **[B.1.1]** Levels = <span style="color: #FF0000">Class=Control</span> < <span style="color: #FF0000">Class=Impaired</span>
|      **[B.2]** 127/128 predictors = All remaining variables (3/127 factor + 124/127 numeric)
|
| **[C]** No pre-processing actions applied:
|      **[C.1]** No shape transformation applied since distributions were fairly normal
|      **[C.2]** Centering and scaling may be necessary due to the differences in ranges for the numeric variables however, these will be selectively applied based on model requirements
|      **[C.2]** No outlier treatment applied since the high values noted were minimal and contextually valid
|      **[C.3]** No predictors removed due to zero or near-zero variance 
|      **[C.4]** No predictors removed due to high correlation
|      **[C.5]** No predictors removed due to linear dependencies
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.7, warning=FALSE, message=FALSE}
##################################
# Creating the pre-modelling
# train set
##################################
PMA_PreModelling_Train <- Alzheimer_Train
PMA_PreModelling_Train$Class <- as.factor(PMA_PreModelling_Train$Class)
PMA_PreModelling_Train$E2 <- as.factor(PMA_PreModelling_Train$E2)
PMA_PreModelling_Train$E3 <- as.factor(PMA_PreModelling_Train$E3)
PMA_PreModelling_Train$E4 <- as.factor(PMA_PreModelling_Train$E4)

##################################
# Gathering descriptive statistics
##################################
(PMA_PreModelling_Train_Skimmed <- skim(PMA_PreModelling_Train))

###################################
# Verifying the data dimensions
# for the train set
###################################
dim(PMA_PreModelling_Train)

##################################
# Formulating the test set
##################################
PMA_PreModelling_Test <- Alzheimer_Test
PMA_PreModelling_Test$Class <- as.factor(PMA_PreModelling_Test$Class)
PMA_PreModelling_Test$E2 <- as.factor(PMA_PreModelling_Test$E2)
PMA_PreModelling_Test$E3 <- as.factor(PMA_PreModelling_Test$E3)
PMA_PreModelling_Test$E4 <- as.factor(PMA_PreModelling_Test$E4)

##################################
# Gathering descriptive statistics
##################################
(PMA_PreModelling_Test_Skimmed <- skim(PMA_PreModelling_Test))

###################################
# Verifying the data dimensions
# for the test set
###################################
dim(PMA_PreModelling_Test)

```

</details>

##  1.4 Data Exploration
|
| **[A]** Numeric variables which demonstrated differential relationships with the <span style="color: #FF0000">Class</span> response variable between its <span style="color: #FF0000">Control</span> and <span style="color: #FF0000">Impaired</span> levels include:
|      **[A.1]** <span style="color: #FF0000">Fibrinogen</span> variable (numeric)
|      **[A.2]** <span style="color: #FF0000">GRO_alpha</span> variable (numeric)
|      **[A.3]** <span style="color: #FF0000">FAS</span> variable (numeric)
|      **[A.4]** <span style="color: #FF0000">Eotaxin_3</span> variable (numeric)
|      **[A.5]** <span style="color: #FF0000">Creatine_Kinase_MB</span> variable (numeric)
|      **[A.6]** <span style="color: #FF0000">IGF_BP2</span> variable (numeric)
|      **[A.7]** <span style="color: #FF0000">Gamma_Interferon_Induced_Monokin</span> variable (numeric)
|      **[A.8]** <span style="color: #FF0000">MCP_2</span> variable (numeric)
|      **[A.9]** <span style="color: #FF0000">MIF</span> variable (numeric)
|      **[A.10]** <span style="color: #FF0000">Pancreatic_polypteptide</span> variable (numeric)
|      **[A.11]** <span style="color: #FF0000">PAI_1</span> variable (numeric)
|      **[A.12]** <span style="color: #FF0000">NT_proBNP</span> variable (numeric)
|      **[A.13]** <span style="color: #FF0000">MMP10</span> variable (numeric)
|      **[A.14]** <span style="color: #FF0000">MMP7</span> variable (numeric)
|      **[A.15]** <span style="color: #FF0000">TRAIL_R3</span> variable (numeric)
|      **[A.16]** <span style="color: #FF0000">Pulmonary_and_Activation_Regulat</span> variable (numeric)
|      **[A.17]** <span style="color: #FF0000">Resistin</span> variable (numeric)
|      **[A.18]** <span style="color: #FF0000">VEGF</span> variable (numeric)
|      **[A.19]** <span style="color: #FF0000">Thrombopoietin</span> variable (numeric)
|      **[A.20]** <span style="color: #FF0000">Thymus_Expressed_Chemokine_TECK</span> variable (numeric)
|
| **[B]** Factor variables which demonstrated relatively better differentiation of the <span style="color: #FF0000">Class</span> response variable between its <span style="color: #FF0000">Control</span> and <span style="color: #FF0000">Impaired</span> levels include:
|      **[B.1]** <span style="color: #FF0000">E2</span> variable (factor)
|      **[B.2]** <span style="color: #FF0000">E4</span> variable (factor)
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.4, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
EDA <- PMA_PreModelling_Train

##################################
# Listing all predictors
##################################
EDA.Predictors <- EDA[,!names(EDA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
EDA.Predictors.Numeric <- EDA.Predictors[,sapply(EDA.Predictors, is.numeric)]
ncol(EDA.Predictors.Numeric)
names(EDA.Predictors.Numeric)

##################################
# Listing all factor predictors
##################################
EDA.Predictors.Factor <- EDA.Predictors[,sapply(EDA.Predictors, is.factor)]
ncol(EDA.Predictors.Factor)
names(EDA.Predictors.Factor)

##################################
# Formulating the box plots
##################################
featurePlot(x = EDA.Predictors.Numeric[1:124], 
            y = EDA$Class,
            plot = "box",
            scales = list(x = list(relation="free", rot = 90), 
                          y = list(relation="free")),
            adjust = 1.5, 
            pch = "|",
            layout=(c(4,4)))

##################################
# Restructuring the dataset for
# for barchart analysis
##################################
EDA.Bar.Source <- as.data.frame(cbind(EDA$Class,
                                      EDA.Predictors.Factor))
names(EDA.Bar.Source) <- c("Class",names(EDA.Predictors.Factor))
ncol(EDA.Bar.Source)

##################################
# Creating a function to formulate
# the proportions table
##################################
EDA.PropTable.Function <- function(FactorVar) {
  EDA.Bar.Source.FactorVar <- EDA.Bar.Source[,c("Class",
                                                FactorVar)]
  EDA.Bar.Source.FactorVar.Prop <- as.data.frame(prop.table(table(EDA.Bar.Source.FactorVar), 2))
  names(EDA.Bar.Source.FactorVar.Prop)[2] <- "Class"
  EDA.Bar.Source.FactorVar.Prop$Variable <- rep(FactorVar,nrow(EDA.Bar.Source.FactorVar.Prop))

  return(EDA.Bar.Source.FactorVar.Prop)

}

EDA.Bar.Source.FactorVar.Prop <- rbind(EDA.PropTable.Function("E2"),
                                       EDA.PropTable.Function("E3"),
                                       EDA.PropTable.Function("E4"))

(EDA.Barchart.FactorVar <- barchart(EDA.Bar.Source.FactorVar.Prop[,3] ~
                                      EDA.Bar.Source.FactorVar.Prop[,2] | EDA.Bar.Source.FactorVar.Prop[,4],
                                      data=EDA.Bar.Source.FactorVar.Prop,
                                      groups = EDA.Bar.Source.FactorVar.Prop[,1],
                                      stack=TRUE,
                                      ylab = "Proportion",
                                      xlab = "Class",
                                      auto.key = list(adj=1, space="top", columns=2),
                                      layout=(c(3,1))))

```

</details>

##  1.5 Recursive Feature Elimination (RFE)

###  1.5.1 Random Forest Without RFE (RF_FULL)
|
| [Random Forest](https://link.springer.com/content/pdf/10.1007/BF00058655.pdf) is an ensemble learning method made up of a large set of small decision trees called estimators, with each producing its own prediction. The random forest model aggregates the predictions of the estimators to produce a more accurate prediction. The algorithm involves bootstrap aggregating (where smaller subsets of the training data are repeatedly subsampled with replacement), random subspacing (where a subset of features are sampled and used to train each individual estimator), estimator training (where unpruned decision trees are formulated for each estimator) and inference by aggregating the predictions of all estimators.
|
| **[A]** The random forest model from the  <mark style="background-color: #CCECFF">**randomForest**</mark> package was implemented without recursive feature elimination through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model contains 1 hyperparameter:
|      **[B.1]** <span style="color: #FF0000">mtry</span> = number of randomly selected predictors held constant at a value of 11
|
| **[C]** The cross-validated model performance of the final model is summarized as follows:
|      **[C.1]** Final model configuration involves mtry=11
|      **[C.2]** AUROC = 0.78268
|
| **[D]** The model allows for ranking of predictors in terms of variable importance. The top-performing predictors in the model are as follows:
|      **[D.1]** <span style="color: #FF0000">MMP10</span> variable (numeric)
|      **[D.2]** <span style="color: #FF0000">Crystatin_C</span> variable (numeric)
|      **[D.3]** <span style="color: #FF0000">MMP7</span> variable (numeric)
|      **[D.4]** <span style="color: #FF0000">TRAIL_R3</span> variable (numeric)
|      **[D.5]** <span style="color: #FF0000">Pancreatic_polypeptide</span> variable (numeric)
|
| **[E]** The independent test model performance of the final model is summarized as follows:
|      **[E.1]** AUROC = 0.79803
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.5.1, warning=FALSE, message=FALSE}
##################################
# Converting all predictors to numeric
# for both train and test data
##################################
for (i in 1:ncol(PMA_PreModelling_Train)){
  if (names(PMA_PreModelling_Train)[i]!="Class"){
    PMA_PreModelling_Train[,i] <- as.numeric(PMA_PreModelling_Train[,i])
  }
}
summary(PMA_PreModelling_Train)

for (i in 1:ncol(PMA_PreModelling_Test)){
  if (names(PMA_PreModelling_Test)[i]!="Class"){
    PMA_PreModelling_Test[,i] <- as.numeric(PMA_PreModelling_Test[,i])
  }
}
summary(PMA_PreModelling_Test)

##################################
# Formulating a function to summarize
# model performance metrics
##################################
FiveMetricsSummary <- function(...) c(twoClassSummary(...), defaultSummary(...))

##################################
# Creating consistent fold assignments 
# for the Cross Validation process
##################################
set.seed(12345678)
KFold_Indices <- createFolds(PMA_PreModelling_Train$Class ,
                             k = 10,
                             returnTrain=TRUE)

##################################
# Creating a range of
# variable subsets for evaluation
##################################
VariableSubset <- seq(1, length(names(PMA_PreModelling_Train))-2, by=20)

##################################
# Formulating the controls for the 
# recursive feature elimination process
##################################
KFold_RFEControl <- rfeControl(method = "cv",
                               saveDetails = TRUE,
                               index = KFold_Indices,
                               returnResamp = "final")

##################################
# Formulating the controls for the 
# model training process
##################################
KFold_TrainControl <- trainControl(method = "cv",
                                   summaryFunction = FiveMetricsSummary,
                                   classProbs = TRUE,
                                   index = KFold_Indices)

##################################
# Running the random forest model
# by setting the caret method to 'rf'
##################################
set.seed(12345678)
RF_FULL_Tune <- caret::train(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                        y = PMA_PreModelling_Train$Class,
                        method = "rf",
                        metric = "ROC",
                        tuneGrid = data.frame(mtry = floor(sqrt(length(names(PMA_PreModelling_Train) %in% c("Class"))))),
                        ntree = 100,
                        trControl = KFold_TrainControl)

##################################
# Reporting the cross-validation results
# for the train set
##################################
RF_FULL_Tune

RF_FULL_Tune$finalModel

RF_FULL_Tune$results

(RF_FULL_Train_ROCCurveAUC <- RF_FULL_Tune$results[,c("ROC")])

##################################
# Identifying and plotting the
# best model predictors
##################################
RF_FULL_VarImp <- varImp(RF_FULL_Tune, scale = TRUE)
plot(RF_FULL_VarImp,
     top=25,
     scales=list(y=list(cex = .95)),
     main="Ranked Variable Importance : Random Forest",
     xlab="Scaled Variable Importance Metrics",
     ylab="Predictors",
     cex=2,
     origin=0,
     alpha=0.45)

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
RF_FULL_Test <- data.frame(RF_FULL_Observed = PMA_PreModelling_Test$Class,
                      RF_FULL_Predicted = predict(RF_FULL_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

##################################
# Reporting the independent evaluation results
# for the test set
##################################
RF_FULL_Test_ROC <- roc(response = RF_FULL_Test$RF_FULL_Observed,
                        predictor = RF_FULL_Test$RF_FULL_Predicted.Impaired,
                        levels = rev(levels(RF_FULL_Test$RF_FULL_Observed)))

(RF_FULL_Test_ROCCurveAUC <- auc(RF_FULL_Test_ROC)[1])

```

</details>

###  1.5.2 Linear Discriminant Analysis Without RFE (LDA_FULL)
|
| [Linear Discriminant Analysis](https://www.semanticscholar.org/paper/THE-USE-OF-MULTIPLE-MEASUREMENTS-IN-TAXONOMIC-Fisher/ab21376e43ac90a4eafd14f0f02a0c87502b6bbf) finds a linear combination of features that best separates the classes in a data set by projecting the data onto a lower-dimensional space that maximizes the separation between the classes. The algorithm searches for a set of linear discriminants that maximize the ratio of between-class variance to within-class variance by evaluating directions in the feature space that best separate the different classes of data. LDA assumes that the data has a Gaussian distribution and that the covariance matrices of the different classes are equal, in addition to the data being linearly separable by the presence of a linear decision boundary can accurately classify the different classes.
|
| **[A]** The linear discriminant analysis model from the  <mark style="background-color: #CCECFF">**MASS**</mark> package was implemented without recursive feature elimination through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model does not contain any hyperparameter.
|
| **[C]** The cross-validated model performance of the final model is summarized as follows:
|      **[C.1]** Final model configuration is fixed due to the absence of a hyperparameter
|      **[C.2]** AUROC = 0.80151
|
| **[D]** The model does not allow for ranking of predictors in terms of variable importance.
|
| **[E]** The independent test model performance of the final model is summarized as follows:
|      **[E.1]** AUROC = 0.77199
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.5.2, warning=FALSE, message=FALSE}
##################################
# Running the linear discriminant analysis model
# by setting the caret method to 'lda'
##################################
set.seed(12345678)
LDA_FULL_Tune <- caret::train(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                         y = PMA_PreModelling_Train$Class,
                         method = "lda",
                         metric = "ROC",
                         tol = 1.0e-12,
                         trControl = KFold_TrainControl)

##################################
# Reporting the cross-validation results
# for the train set
##################################
LDA_FULL_Tune

(LDA_FULL_Train_ROCCurveAUC <- LDA_FULL_Tune$results[,c("ROC")])

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
LDA_FULL_Test <- data.frame(LDA_FULL_Observed = PMA_PreModelling_Test$Class,
                      LDA_FULL_Predicted = predict(LDA_FULL_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

##################################
# Reporting the independent evaluation results
# for the test set
##################################
LDA_FULL_Test_ROC <- roc(response = LDA_FULL_Test$LDA_FULL_Observed,
                        predictor = LDA_FULL_Test$LDA_FULL_Predicted.Impaired,
                        levels = rev(levels(LDA_FULL_Test$LDA_FULL_Observed)))

(LDA_FULL_Test_ROCCurveAUC <- auc(LDA_FULL_Test_ROC)[1])

```

</details>

###  1.5.3 Naive Bayes Without RFE (NB_FULL)
|
| [Naive Bayes Classifier](https://www.jstor.org/stable/2682766) categorizes instances by applying Bayes Theorem in determining posterior probabilities as conditioned by the likelihood of features, and prior probabilities pertaining to both events and features. The algorithm naively assumes independence between features and assigns the same weight (degree of significance) to all given features.
|
| **[A]** The naive bayes model from the  <mark style="background-color: #CCECFF">**klaR**</mark> package was implemented without recursive feature elimination through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model contains 3 hyperparameters:
|      **[B.1]** <span style="color: #FF0000">fL</span> = laplace correction held constant at a value of 0
|      **[B.2]** <span style="color: #FF0000">adjust</span> = bandwidth adjustment held constant at a value of TRUE
|      **[B.3]** <span style="color: #FF0000">usekernel</span> = distribution type made to vary across a range of levels equal to TRUE and FALSE
|
| **[C]** The cross-validated model performance of the final model is summarized as follows:
|      **[C.1]** Final model configuration involves fL=0, adjust=TRUE and usekernel=TRUE
|      **[C.2]** AUROC = 0.73904
|
| **[D]** The model does not allow for ranking of predictors in terms of variable importance.
|
| **[E]** The independent test model performance of the final model is summarized as follows:
|      **[E.1]** AUROC = 0.68055
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.5.3, warning=FALSE, message=FALSE}
##################################
# Running the naive bayes model
# by setting the caret method to 'nb'
##################################
set.seed(12345678)
NB_FULL_Tune <- caret::train(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                        y = PMA_PreModelling_Train$Class,
                        method = "nb",
                        metric = "ROC",
                        trControl = KFold_TrainControl)

##################################
# Reporting the cross-validation results
# for the train set
##################################
NB_FULL_Tune

NB_FULL_Tune$finalModel

NB_FULL_Tune$results

(NB_FULL_Train_ROCCurveAUC <- NB_FULL_Tune$results[NB_FULL_Tune$results$usekernel==NB_FULL_Tune$bestTune$usekernel,
                                                   c("ROC")])

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
NB_FULL_Test <- data.frame(NB_FULL_Observed = PMA_PreModelling_Test$Class,
                      NB_FULL_Predicted = predict(NB_FULL_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

##################################
# Reporting the independent evaluation results
# for the test set
##################################
NB_FULL_Test_ROC <- roc(response = NB_FULL_Test$NB_FULL_Observed,
                        predictor = NB_FULL_Test$NB_FULL_Predicted.Impaired,
                        levels = rev(levels(NB_FULL_Test$NB_FULL_Observed)))

(NB_FULL_Test_ROCCurveAUC <- auc(NB_FULL_Test_ROC)[1])


```

</details>

###  1.5.4 Logistic Regression Without RFE (LR_FULL)
|
| [Logistic Regression](http://dx.doi.org/10.2139/ssrn.360300) models the relationship between the probability of an event (among two outcome levels) by having the log-odds of the event be a linear combination of a set of predictors weighted by their respective parameter estimates. The parameters are estimated via maximum likelihood estimation by testing different values through multiple iterations to optimize for the best fit of log odds. All of these iterations produce the log likelihood function, and logistic regression seeks to maximize this function to find the best parameter estimates. Given the optimal parameters, the conditional probabilities for each observation can be calculated, logged, and summed together to yield a predicted probability.
|
| **[A]** The logistic regression model from the  <mark style="background-color: #CCECFF">**stats**</mark> package was implemented without recursive feature elimination through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model does not contain any hyperparameter.
|
| **[C]** The cross-validated model performance of the final model is summarized as follows:
|      **[C.1]** Final model configuration is fixed due to the absence of a hyperparameter
|      **[C.2]** AUROC = 0.70733
|
| **[D]** The model allows for ranking of predictors in terms of variable importance. The top-performing predictors in the model are as follows:
|      **[D.1]** <span style="color: #FF0000">Cortisol</span> variable (numeric)
|      **[D.2]** <span style="color: #FF0000">Apolipoprotein_A1</span> variable (numeric)
|      **[D.3]** <span style="color: #FF0000">MCP_2</span> variable (numeric)
|      **[D.4]** <span style="color: #FF0000">SOD</span> variable (numeric)
|      **[D.5]** <span style="color: #FF0000">Fatty_Acid_Binding_Protein</span> variable (numeric)
|
| **[E]** The independent test model performance of the final model is summarized as follows:
|      **[E.1]** AUROC = 0.77199
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.5.4, warning=FALSE, message=FALSE}
##################################
# Running the logistic regression model
# by setting the caret method to 'glm'
##################################
set.seed(12345678)
LR_FULL_Tune <- caret::train(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                        y = PMA_PreModelling_Train$Class,
                        method = "glm",
                        metric = "ROC",
                        trControl = KFold_TrainControl)

##################################
# Reporting the cross-validation results
# for the train set
##################################
LR_FULL_Tune

LR_FULL_Tune$finalModel

LR_FULL_Tune$results

(LR_FULL_Train_ROCCurveAUC <- LR_FULL_Tune$results[,c("ROC")])

##################################
# Identifying and plotting the
# best model predictors
##################################
LR_FULL_VarImp <- varImp(LR_FULL_Tune, scale = TRUE)
plot(LR_FULL_VarImp,
     top=25,
     scales=list(y=list(cex = .95)),
     main="Ranked Variable Importance : Logistic Regression",
     xlab="Scaled Variable Importance Metrics",
     ylab="Predictors",
     cex=2,
     origin=0,
     alpha=0.45)

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
LR_FULL_Test <- data.frame(LR_FULL_Observed = PMA_PreModelling_Test$Class,
                      LR_FULL_Predicted = predict(LR_FULL_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

##################################
# Reporting the independent evaluation results
# for the test set
##################################
LR_FULL_Test_ROC <- roc(response = LR_FULL_Test$LR_FULL_Observed,
                        predictor = LR_FULL_Test$LR_FULL_Predicted.Impaired,
                        levels = rev(levels(LR_FULL_Test$LR_FULL_Observed)))

(LR_FULL_Test_ROCCurveAUC <- auc(LR_FULL_Test_ROC)[1])

```

</details>

###  1.5.5 Support Vector Machine - Radial Basis Function Kernel Without RFE (SVM_R_FULL)
|
| [Support Vector Machine](http://www.cs.cmu.edu/~pakyan/compbio/references/Drucker_NIPS_1996.pdf) plots each observation in an N-dimensional space corresponding to the number of features in the data set and finds a hyperplane that maximally separates the different classes by a maximally large margin (which is defined as the distance between the hyperplane and the closest data points from each class). The algorithm applies kernel transformation by mapping non-linearly separable data using the similarities between the points in a high-dimensional feature space for improved discrimination.
|
| **[A]** The support vector machine (radial basis function kernel) model from the  <mark style="background-color: #CCECFF">**kernlab**</mark> package was implemented without recursive feature elimination through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model contains 2 hyperparameters:
|      **[B.1]** <span style="color: #FF0000">sigma</span> = sigma held constant at a value of 0.00455
|      **[B.2]** <span style="color: #FF0000">C</span> = cost made to vary across a range of 10 default values
|
| **[C]** The cross-validated model performance of the final model is summarized as follows:
|      **[C.1]** Final model configuration involves sigma=0.00455 and C=8
|      **[C.2]** AUROC = 0.85690
|
| **[D]** The model does not allow for ranking of predictors in terms of variable importance.
|
| **[E]** The independent test model performance of the final model is summarized as follows:
|      **[E.1]** AUROC = 0.82060
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.5.5, warning=FALSE, message=FALSE}
##################################
# Running the support vector machine (radial basis function kernel) model
# by setting the caret method to 'svmRadial'
##################################
set.seed(12345678)
SVM_R_FULL_Tune <- caret::train(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                           y = PMA_PreModelling_Train$Class,
                           method = "svmRadial",
                           metric = "ROC",
                           tuneLength = 10,
                           preProc = c("center", "scale"),
                           trControl = KFold_TrainControl)

##################################
# Reporting the cross-validation results
# for the train set
##################################
SVM_R_FULL_Tune

SVM_R_FULL_Tune$finalModel

SVM_R_FULL_Tune$results

(SVM_R_FULL_Train_ROCCurveAUC <- SVM_R_FULL_Tune$results[SVM_R_FULL_Tune$results$C==SVM_R_FULL_Tune$bestTune$C,
                                                         c("ROC")])

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
SVM_R_FULL_Test <- data.frame(SVM_R_FULL_Observed = PMA_PreModelling_Test$Class,
                      SVM_R_FULL_Predicted = predict(SVM_R_FULL_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

##################################
# Reporting the independent evaluation results
# for the test set
##################################
SVM_R_FULL_Test_ROC <- roc(response = SVM_R_FULL_Test$SVM_R_FULL_Observed,
                        predictor = SVM_R_FULL_Test$SVM_R_FULL_Predicted.Impaired,
                        levels = rev(levels(SVM_R_FULL_Test$SVM_R_FULL_Observed)))

(SVM_R_FULL_Test_ROCCurveAUC <- auc(SVM_R_FULL_Test_ROC)[1])

```

</details>

###  1.5.6 K-Nearest Neighbors Without RFE (KNN_FULL)
|
| [K-Nearest Neighbors](https://isl.stanford.edu/~cover/papers/transIT/0021cove.pdf) works on the similarity principle which assumes that every data point falling in near to each other belong to the same class. The algorithm therefore assigns an unclassified sample point the classification of the nearest of a set of previously classified points.
|
| **[A]** The k-nearest neighbors model was implemented without recursive feature elimination through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model contains 1 hyperparameter:
|      **[B.1]** <span style="color: #FF0000">k</span> = number of neighbors made to vary across a range of 10 default values
|
| **[C]** The cross-validated model performance of the final model is summarized as follows:
|      **[C.1]** Final model configuration involves k=23
|      **[C.2]** AUROC = 0.79886
|
| **[D]** The model does not allow for ranking of predictors in terms of variable importance.
|
| **[E]** The independent test model performance of the final model is summarized as follows:
|      **[E.1]** AUROC = 0.79167
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.5.6, warning=FALSE, message=FALSE}
##################################
# Running the k-nearest neighbors model
# by setting the caret method to 'knn'
##################################
set.seed(12345678)
KNN_FULL_Tune <- caret::train(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                         y = PMA_PreModelling_Train$Class,
                         method = "knn",
                         metric = "ROC",
                         tuneLength = 10,
                         preProc = c("center", "scale"),
                         trControl = KFold_TrainControl)

##################################
# Reporting the cross-validation results
# for the train set
##################################
KNN_FULL_Tune

KNN_FULL_Tune$finalModel

KNN_FULL_Tune$results

(KNN_FULL_Train_ROCCurveAUC <- KNN_FULL_Tune$results[KNN_FULL_Tune$results$k==KNN_FULL_Tune$bestTune$k,
                                                         c("ROC")])

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
KNN_FULL_Test <- data.frame(KNN_FULL_Observed = PMA_PreModelling_Test$Class,
                      KNN_FULL_Predicted = predict(KNN_FULL_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

##################################
# Reporting the independent evaluation results
# for the test set
##################################
KNN_FULL_Test_ROC <- roc(response = KNN_FULL_Test$KNN_FULL_Observed,
                        predictor = KNN_FULL_Test$KNN_FULL_Predicted.Impaired,
                        levels = rev(levels(KNN_FULL_Test$KNN_FULL_Observed)))

(KNN_FULL_Test_ROCCurveAUC <- auc(KNN_FULL_Test_ROC)[1])

```

</details>

###  1.5.7 Random Forest With RFE (RF_RFE)
|
| [Random Forest](https://link.springer.com/content/pdf/10.1007/BF00058655.pdf) is an ensemble learning method made up of a large set of small decision trees called estimators, with each producing its own prediction. The random forest model aggregates the predictions of the estimators to produce a more accurate prediction. The algorithm involves bootstrap aggregating (where smaller subsets of the training data are repeatedly subsampled with replacement), random subspacing (where a subset of features are sampled and used to train each individual estimator), estimator training (where unpruned decision trees are formulated for each estimator) and inference by aggregating the predictions of all estimators.
|
| [Recursive Feature Elimination](https://ieeexplore.ieee.org/document/4457268) is a wrapper-style feature selection algorithm which searches for a subset of features by starting with all features in the training data set and successfully removing features until the desired number remains. The algorithm repeatedly fits a given machine learning algorithm used in the core of the model, ranks features by importance, discards the least important features, and re-fits the model. Features are scored either using importance scores relevant to the provided machine learning model or by applying statistical methods.
|
| **[A]** The random forest model from the  <mark style="background-color: #CCECFF">**randomForest**</mark> package was implemented with recursive feature elimination through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model contains 1 hyperparameter:
|      **[B.1]** <span style="color: #FF0000">mtry</span> = number of randomly selected predictors held constant at a value of 6
|
| **[C]** Recursive feature elimination was applied across a range of variable subset sizes ranging from 1 to 127:
|      **[C.1]** The variable subset with the best cross-validated performance was 41 with the top 5 variables identified as:
|             **[C.1.1]** <span style="color: #FF0000">MMP10</span> variable (numeric)
|             **[C.1.2]** <span style="color: #FF0000">Cystatin_C</span> variable (numeric)
|             **[C.1.3]** <span style="color: #FF0000">TRAIL_R3</span> variable (numeric)
|             **[C.1.4]** <span style="color: #FF0000">PAI_1</span> variable (numeric)
|             **[C.1.5]** <span style="color: #FF0000">GRO_alpha</span> variable (numeric)
|
| **[D]** The cross-validated model performance of the final model is summarized as follows:
|      **[D.1]** Final model configuration involves mtry=11 and variable subset=41
|      **[D.2]** AUROC = 0.81534
|
| **[E]** The independent test model performance of the final model is summarized as follows:
|      **[E.1]** AUROC = 0.84201
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.5.7, warning=FALSE, message=FALSE}
##################################
# Running the random forest model
# by setting the caret method to 'rf'
# with implementation of recursive feature elimination
##################################
KFold_RFEControl$functions <- rfFuncs
KFold_RFEControl$functions$summary <- FiveMetricsSummary

set.seed(12345678)
RF_RFE_Tune <- caret::rfe(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                          y = PMA_PreModelling_Train$Class,
                          sizes = VariableSubset,
                          metric = "ROC",
                          ntree = 100,
                          rfeControl = KFold_RFEControl)

##################################
# Reporting the cross-validation results
# for the train set
##################################
RF_RFE_Tune

RF_RFE_Tune$fit

RF_RFE_Tune$results

(RF_RFE_Train_ROCCurveAUC <- RF_RFE_Tune$results[RF_RFE_Tune$results$ROC==max(RF_RFE_Tune$results$ROC),
                                                       c("ROC")])

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
RF_RFE_Test <- data.frame(RF_RFE_Observed = PMA_PreModelling_Test$Class,
                      RF_RFE_Predicted = predict(RF_RFE_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

##################################
# Reporting the independent evaluation results
# for the test set
##################################
RF_RFE_Test_ROC <- roc(response = RF_RFE_Test$RF_RFE_Observed,
                        predictor = RF_RFE_Test$RF_RFE_Predicted.Impaired,
                        levels = rev(levels(RF_RFE_Test$RF_RFE_Observed)))

(RF_RFE_Test_ROCCurveAUC <- auc(RF_RFE_Test_ROC)[1])

```

</details>

###  1.5.8 Linear Discriminant Analysis With RFE (LDA_RFE)
|
| [Linear Discriminant Analysis](https://www.semanticscholar.org/paper/THE-USE-OF-MULTIPLE-MEASUREMENTS-IN-TAXONOMIC-Fisher/ab21376e43ac90a4eafd14f0f02a0c87502b6bbf) finds a linear combination of features that best separates the classes in a data set by projecting the data onto a lower-dimensional space that maximizes the separation between the classes. The algorithm searches for a set of linear discriminants that maximize the ratio of between-class variance to within-class variance by evaluating directions in the feature space that best separate the different classes of data. LDA assumes that the data has a Gaussian distribution and that the covariance matrices of the different classes are equal, in addition to the data being linearly separable by the presence of a linear decision boundary can accurately classify the different classes.
|
| [Recursive Feature Elimination](https://ieeexplore.ieee.org/document/4457268) is a wrapper-style feature selection algorithm which searches for a subset of features by starting with all features in the training data set and successfully removing features until the desired number remains. The algorithm repeatedly fits a given machine learning algorithm used in the core of the model, ranks features by importance, discards the least important features, and re-fits the model. Features are scored either using importance scores relevant to the provided machine learning model or by applying statistical methods.
|
| **[A]** The linear discriminant analysis model from the  <mark style="background-color: #CCECFF">**MASS**</mark> package was implemented with recursive feature elimination through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model does not contain any hyperparameter.
|
| **[C]** Recursive feature elimination was applied across a range of variable subset sizes ranging from 1 to 127:
|      **[C.1]** The variable subset with the best cross-validated performance was 81 with the top 5 variables identified as:
|             **[C.1.1]** <span style="color: #FF0000">MMP10</span> variable (numeric)
|             **[C.1.2]** <span style="color: #FF0000">GRO_alpha</span> variable (numeric)
|             **[C.1.3]** <span style="color: #FF0000">TRAIL_R3</span> variable (numeric)
|             **[C.1.4]** <span style="color: #FF0000">Fibrinogen</span> variable (numeric)
|             **[C.1.5]** <span style="color: #FF0000">PAI_1</span> variable (numeric)
|
| **[D]** The cross-validated model performance of the final model is summarized as follows:
|      **[D.1]** Final model configuration involves variable subset=81
|      **[D.2]** AUROC = 0.83569
|
| **[E]** The independent test model performance of the final model is summarized as follows:
|      **[E.1]** AUROC = 0.85301
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.5.8, warning=FALSE, message=FALSE}
##################################
# Running the linear discriminant analysis model
# by setting the caret method to 'lda'
# with implementation of recursive feature elimination
##################################
KFold_RFEControl$functions <- ldaFuncs
KFold_RFEControl$functions$summary <- FiveMetricsSummary

set.seed(12345678)
LDA_RFE_Tune <- caret::rfe(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                           y = PMA_PreModelling_Train$Class,
                           sizes = VariableSubset,
                           metric = "ROC",
                           tol = 1.0e-12,
                           rfeControl = KFold_RFEControl)

##################################
# Reporting the cross-validation results
# for the train set
##################################
LDA_RFE_Tune

LDA_RFE_Tune$fit

LDA_RFE_Tune$results

(LDA_RFE_Train_ROCCurveAUC <- LDA_RFE_Tune$results[LDA_RFE_Tune$results$ROC==max(LDA_RFE_Tune$results$ROC),
                                                       c("ROC")])

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
LDA_RFE_Test <- data.frame(LDA_RFE_Observed = PMA_PreModelling_Test$Class,
                      LDA_RFE_Predicted = predict(LDA_RFE_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

##################################
# Reporting the independent evaluation results
# for the test set
##################################
LDA_RFE_Test_ROC <- roc(response = LDA_RFE_Test$LDA_RFE_Observed,
                        predictor = LDA_RFE_Test$LDA_RFE_Predicted.Impaired,
                        levels = rev(levels(LDA_RFE_Test$LDA_RFE_Observed)))

(LDA_RFE_Test_ROCCurveAUC <- auc(LDA_RFE_Test_ROC)[1])

```

</details>

###  1.5.9 Naive Bayes With RFE (NB_RFE)
|
| [Naive Bayes Classifier](https://www.jstor.org/stable/2682766) categorizes instances by applying Bayes Theorem in determining posterior probabilities as conditioned by the likelihood of features, and prior probabilities pertaining to both events and features. The algorithm naively assumes independence between features and assigns the same weight (degree of significance) to all given features.
|
| [Recursive Feature Elimination](https://ieeexplore.ieee.org/document/4457268) is a wrapper-style feature selection algorithm which searches for a subset of features by starting with all features in the training data set and successfully removing features until the desired number remains. The algorithm repeatedly fits a given machine learning algorithm used in the core of the model, ranks features by importance, discards the least important features, and re-fits the model. Features are scored either using importance scores relevant to the provided machine learning model or by applying statistical methods.
|
| **[A]** The naive bayes model from the  <mark style="background-color: #CCECFF">**klaR**</mark> package was implemented with recursive feature elimination through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model contains 3 hyperparameters:
|      **[B.1]** <span style="color: #FF0000">fL</span> = laplace correction held constant at a value of 0
|      **[B.2]** <span style="color: #FF0000">adjust</span> = bandwidth adjustment held constant at a value of TRUE
|      **[B.3]** <span style="color: #FF0000">usekernel</span> = distribution type made to vary across a range of levels equal to TRUE and FALSE
|
| **[C]** Recursive feature elimination was applied across a range of variable subset sizes ranging from 1 to 127:
|      **[C.1]** The variable subset with the best cross-validated performance was 21 with the top 5 variables identified as:
|             **[C.1.1]** <span style="color: #FF0000">MMP10</span> variable (numeric)
|             **[C.1.2]** <span style="color: #FF0000">GRO_alpha</span> variable (numeric)
|             **[C.1.3]** <span style="color: #FF0000">TRAIL_R3</span> variable (numeric)
|             **[C.1.4]** <span style="color: #FF0000">Fibrinogen</span> variable (numeric)
|             **[C.1.5]** <span style="color: #FF0000">PAI_1</span> variable (numeric)
|
| **[D]** The cross-validated model performance of the final model is summarized as follows:
|      **[D.1]** Final model configuration involves fL=0, adjust=TRUE, usekernel=TRUE and variable subset=21
|      **[D.2]** AUROC = 0.76825
|
| **[E]** The independent test model performance of the final model is summarized as follows:
|      **[E.1]** AUROC = 0.75231
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.5.9, warning=FALSE, message=FALSE}
##################################
# Running the naive bayes model
# by setting the caret method to 'nb'
# with implementation of recursive feature elimination
##################################
KFold_RFEControl$functions <- nbFuncs
KFold_RFEControl$functions$summary <- FiveMetricsSummary

set.seed(12345678)
NB_RFE_Tune <- caret::rfe(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                          y = PMA_PreModelling_Train$Class,
                          sizes = VariableSubset,
                          metric = "ROC",
                          rfeControl = KFold_RFEControl)

##################################
# Reporting the cross-validation results
# for the train set
##################################
NB_RFE_Tune

NB_RFE_Tune$fit

NB_RFE_Tune$results

(NB_RFE_Train_ROCCurveAUC <- NB_RFE_Tune$results[NB_RFE_Tune$results$ROC==max(NB_RFE_Tune$results$ROC),
                                                       c("ROC")])

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
NB_RFE_Test <- data.frame(NB_RFE_Observed = PMA_PreModelling_Test$Class,
                      NB_RFE_Predicted = predict(NB_RFE_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

##################################
# Reporting the independent evaluation results
# for the test set
##################################
NB_RFE_Test_ROC <- roc(response = NB_RFE_Test$NB_RFE_Observed,
                        predictor = NB_RFE_Test$NB_RFE_Predicted.Impaired,
                        levels = rev(levels(NB_RFE_Test$NB_RFE_Observed)))

(NB_RFE_Test_ROCCurveAUC <- auc(NB_RFE_Test_ROC)[1])

```

</details>

###  1.5.10 Logistic Regression With RFE (LR_RFE)
|
| [Logistic Regression](http://dx.doi.org/10.2139/ssrn.360300) models the relationship between the probability of an event (among two outcome levels) by having the log-odds of the event be a linear combination of a set of predictors weighted by their respective parameter estimates. The parameters are estimated via maximum likelihood estimation by testing different values through multiple iterations to optimize for the best fit of log odds. All of these iterations produce the log likelihood function, and logistic regression seeks to maximize this function to find the best parameter estimates. Given the optimal parameters, the conditional probabilities for each observation can be calculated, logged, and summed together to yield a predicted probability.
|
| [Recursive Feature Elimination](https://ieeexplore.ieee.org/document/4457268) is a wrapper-style feature selection algorithm which searches for a subset of features by starting with all features in the training data set and successfully removing features until the desired number remains. The algorithm repeatedly fits a given machine learning algorithm used in the core of the model, ranks features by importance, discards the least important features, and re-fits the model. Features are scored either using importance scores relevant to the provided machine learning model or by applying statistical methods.
|
| **[A]** The logistic regression model from the  <mark style="background-color: #CCECFF">**stats**</mark> package was implemented with recursive feature elimination through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model does not contain any hyperparameter.
|
| **[C]** Recursive feature elimination was applied across a range of variable subset sizes ranging from 1 to 127:
|      **[C.1]** The variable subset with the best cross-validated performance was 21 with the top 5 variables identified as:
|             **[C.1.1]** <span style="color: #FF0000">Cortisol</span> variable (numeric)
|             **[C.1.2]** <span style="color: #FF0000">RANTES</span> variable (numeric)
|             **[C.1.3]** <span style="color: #FF0000">I_309</span> variable (numeric)
|             **[C.1.4]** <span style="color: #FF0000">MCP_2</span> variable (numeric)
|             **[C.1.5]** <span style="color: #FF0000">Clusterin_Apo_J</span> variable (numeric)
|
| **[D]** The cross-validated model performance of the final model is summarized as follows:
|      **[D.1]** Final model configuration variable subset=21
|      **[D.2]** AUROC = 0.80296
|
| **[E]** The independent test model performance of the final model is summarized as follows:
|      **[E.1]** AUROC = 0.86921
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.5.10, warning=FALSE, message=FALSE}
##################################
# Running the logistic regression model
# by setting the caret method to 'glm'
# with implementation of recursive feature elimination
##################################
KFold_RFEControl$functions <- lrFuncs
KFold_RFEControl$functions$summary <- FiveMetricsSummary

set.seed(12345678)
LR_RFE_Tune <- caret::rfe(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                          y = PMA_PreModelling_Train$Class,
                          sizes = VariableSubset,
                          metric = "ROC",
                          rfeControl = KFold_RFEControl)

##################################
# Reporting the cross-validation results
# for the train set
##################################
LR_RFE_Tune

LR_RFE_Tune$fit

LR_RFE_Tune$results

(LR_RFE_Train_ROCCurveAUC <- LR_RFE_Tune$results[LR_RFE_Tune$results$ROC==max(LR_RFE_Tune$results$ROC),
                                                       c("ROC")])

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
LR_RFE_Test <- data.frame(LR_RFE_Observed = PMA_PreModelling_Test$Class,
                      LR_RFE_Predicted = predict(LR_RFE_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

##################################
# Reporting the independent evaluation results
# for the test set
##################################
LR_RFE_Test_ROC <- roc(response = LR_RFE_Test$LR_RFE_Observed,
                        predictor = LR_RFE_Test$LR_RFE_Predicted.Impaired,
                        levels = rev(levels(LR_RFE_Test$LR_RFE_Observed)))

(LR_RFE_Test_ROCCurveAUC <- auc(LR_RFE_Test_ROC)[1])

```

</details>

###  1.5.11 Support Vector Machine - Radial Basis Function Kernel With RFE (SVM_R_RFE)
|
| [Support Vector Machine](http://www.cs.cmu.edu/~pakyan/compbio/references/Drucker_NIPS_1996.pdf) plots each observation in an N-dimensional space corresponding to the number of features in the data set and finds a hyperplane that maximally separates the different classes by a maximally large margin (which is defined as the distance between the hyperplane and the closest data points from each class). The algorithm applies kernel transformation by mapping non-linearly separable data using the similarities between the points in a high-dimensional feature space for improved discrimination.
|
| [Recursive Feature Elimination](https://ieeexplore.ieee.org/document/4457268) is a wrapper-style feature selection algorithm which searches for a subset of features by starting with all features in the training data set and successfully removing features until the desired number remains. The algorithm repeatedly fits a given machine learning algorithm used in the core of the model, ranks features by importance, discards the least important features, and re-fits the model. Features are scored either using importance scores relevant to the provided machine learning model or by applying statistical methods.
|
| **[A]** The support vector machine (radial basis function kernel) model from the  <mark style="background-color: #CCECFF">**kernlab**</mark> package was implemented with recursive feature elimination through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model contains 2 hyperparameters:
|      **[B.1]** <span style="color: #FF0000">sigma</span> = sigma held constant at a value of 0.00455
|      **[B.2]** <span style="color: #FF0000">C</span> = cost made to vary across a range of 10 default values
|
| **[C]** Recursive feature elimination was applied across a range of variable subset sizes ranging from 1 to 127:
|      **[C.1]** The variable subset with the best cross-validated performance was 121 with the top 5 variables identified as:
|             **[C.1.1]** <span style="color: #FF0000">MMP10</span> variable (numeric)
|             **[C.1.2]** <span style="color: #FF0000">GRO_alpha</span> variable (numeric)
|             **[C.1.3]** <span style="color: #FF0000">TRAIL_R3</span> variable (numeric)
|             **[C.1.4]** <span style="color: #FF0000">Fibrinogen</span> variable (numeric)
|             **[C.1.5]** <span style="color: #FF0000">PAI_1</span> variable (numeric)
|
| **[D]** The cross-validated model performance of the final model is summarized as follows:
|      **[D.1]** Final model configuration involves sigma=0.00455, C=8 and variable subset=121
|      **[D.2]** AUROC = 0.84155
|
| **[E]** The independent test model performance of the final model is summarized as follows:
|      **[E.1]** AUROC = 0.85995
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.5.11, warning=FALSE, message=FALSE}
##################################
# Running the support vector machine (radial basis function kernel) model
# by setting the caret method to 'svmRadial'
# with implementation of recursive feature elimination
##################################
KFold_RFEControl$functions <- caretFuncs
KFold_RFEControl$functions$summary <- FiveMetricsSummary

KFold_RFETrainControl <- trainControl(method = "cv",
                                      verboseIter = FALSE,
                                      classProbs = TRUE,
                                      allowParallel = FALSE)

set.seed(12345678)
SVM_R_RFE_Tune <- caret::rfe(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                             y = PMA_PreModelling_Train$Class,
                             method = "svmRadial",
                             metric = "ROC",
                             tuneLength = 10,
                             preProc = c("center", "scale"),
                             trControl = KFold_RFETrainControl,
                             sizes = VariableSubset,
                             rfeControl = KFold_RFEControl)

##################################
# Reporting the cross-validation results
# for the train set
##################################
SVM_R_RFE_Tune

SVM_R_RFE_Tune$fit

SVM_R_RFE_Tune$results

(SVM_R_RFE_Train_ROCCurveAUC <- SVM_R_RFE_Tune$results[SVM_R_RFE_Tune$results$ROC==max(SVM_R_RFE_Tune$results$ROC),
                                                       c("ROC")])

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
SVM_R_RFE_Test <- data.frame(SVM_R_RFE_Observed = PMA_PreModelling_Test$Class,
                      SVM_R_RFE_Predicted = predict(SVM_R_RFE_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

##################################
# Reporting the independent evaluation results
# for the test set
##################################
SVM_R_RFE_Test_ROC <- roc(response = SVM_R_RFE_Test$SVM_R_RFE_Observed,
                        predictor = SVM_R_RFE_Test$SVM_R_RFE_Predicted.Impaired,
                        levels = rev(levels(SVM_R_RFE_Test$SVM_R_RFE_Observed)))

(SVM_R_RFE_Test_ROCCurveAUC <- auc(SVM_R_RFE_Test_ROC)[1])

```

</details>

###  1.5.12 K-Nearest Neighbors With RFE (KNN_RFE)
|
| [K-Nearest Neighbors](https://isl.stanford.edu/~cover/papers/transIT/0021cove.pdf) works on the similarity principle which assumes that every data point falling in near to each other belong to the same class. The algorithm therefore assigns an unclassified sample point the classification of the nearest of a set of previously classified points.
|
| [Recursive Feature Elimination](https://ieeexplore.ieee.org/document/4457268) is a wrapper-style feature selection algorithm which searches for a subset of features by starting with all features in the training data set and successfully removing features until the desired number remains. The algorithm repeatedly fits a given machine learning algorithm used in the core of the model, ranks features by importance, discards the least important features, and re-fits the model. Features are scored either using importance scores relevant to the provided machine learning model or by applying statistical methods.
|
| **[A]** The k-nearest neighbors model was implemented with recursive feature elimination through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model contains 1 hyperparameter:
|      **[B.1]** <span style="color: #FF0000">k</span> = number of neighbors made to vary across a range of 10 default values
|
| **[C]** Recursive feature elimination was applied across a range of variable subset sizes ranging from 1 to 127:
|      **[C.1]** The variable subset with the best cross-validated performance was 41 with the top 5 variables identified as:
|             **[C.1.1]** <span style="color: #FF0000">MMP10</span> variable (numeric)
|             **[C.1.2]** <span style="color: #FF0000">GRO_alpha</span> variable (numeric)
|             **[C.1.3]** <span style="color: #FF0000">TRAIL_R3</span> variable (numeric)
|             **[C.1.4]** <span style="color: #FF0000">Fibrinogen</span> variable (numeric)
|             **[C.1.5]** <span style="color: #FF0000">PAI_1</span> variable (numeric)
|
| **[D]** The cross-validated model performance of the final model is summarized as follows:
|      **[D.1]** Final model configuration involves k=23 and variable subset=41
|      **[D.2]** AUROC = 0.79215
|
| **[E]** The independent test model performance of the final model is summarized as follows:
|      **[E.1]** AUROC = 0.84143
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.5.12, warning=FALSE, message=FALSE}
##################################
# Running the k-nearest neighbors model
# by setting the caret method to 'knn'
# with implementation of recursive feature elimination
##################################
KFold_RFEControl$functions <- caretFuncs
KFold_RFEControl$functions$summary <- FiveMetricsSummary

KFold_RFETrainControl <- trainControl(method = "cv",
                                      verboseIter = FALSE,
                                      classProbs = TRUE,
                                      allowParallel = FALSE)

set.seed(12345678)
KNN_RFE_Tune <- caret::rfe(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                             y = PMA_PreModelling_Train$Class,
                             method = "knn",
                             metric = "ROC",
                             tuneLength = 10,
                             preProc = c("center", "scale"),
                             trControl = KFold_RFETrainControl,
                             sizes = VariableSubset,
                             rfeControl = KFold_RFEControl)

##################################
# Reporting the cross-validation results
# for the train set
##################################
KNN_RFE_Tune

KNN_RFE_Tune$fit

KNN_RFE_Tune$results

(KNN_RFE_Train_ROCCurveAUC <- KNN_RFE_Tune$results[KNN_RFE_Tune$results$ROC==max(KNN_RFE_Tune$results$ROC),
                                                       c("ROC")])

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
KNN_RFE_Test <- data.frame(KNN_RFE_Observed = PMA_PreModelling_Test$Class,
                      KNN_RFE_Predicted = predict(KNN_RFE_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

##################################
# Reporting the independent evaluation results
# for the test set
##################################
KNN_RFE_Test_ROC <- roc(response = KNN_RFE_Test$KNN_RFE_Observed,
                        predictor = KNN_RFE_Test$KNN_RFE_Predicted.Impaired,
                        levels = rev(levels(KNN_RFE_Test$KNN_RFE_Observed)))

(KNN_RFE_Test_ROCCurveAUC <- auc(KNN_RFE_Test_ROC)[1])

```

</details>

##  1.6 Consolidated Findings
|
| **[A]** Models which applied recursive feature elimination to select a subset of informative predictors performed better than those which utilized the full set of predictors.
|      **[A.1]** **RF: Random Forest** (<mark style="background-color: #CCECFF">**randomForest**</mark> package)
|             **[A.1.1]** **RF_FULL (Random Forest Without Recursive Feature Elimination)**  
|                      **[A.1.1.1]** Cross-Validation AUROC = 0.78268
|                      **[A.1.1.2]** Test AUROC = 0.79803
|             **[A.1.2]** **RF_RFE (Random Forest With Recursive Feature Elimination)**  
|                      **[A.1.2.1]** Cross-Validation AUROC = 0.81534
|                      **[A.1.2.2]** Test AUROC = 0.84201
|      **[A.2]** **LDA: Linear Discriminant Analysis** (<mark style="background-color: #CCECFF">**MASS**</mark> package)
|             **[A.2.1]** **LDA_FULL (Linear Discriminant Analysis Without Recursive Feature Elimination)**
|                      **[A.2.1.1]** Cross-Validation AUROC = 0.80151
|                      **[A.2.1.2]** Test AUROC = 0.77199
|             **[A.2.2]** **LDA_RFE (Linear Discriminant Analysis With Recursive Feature Elimination)**
|                      **[A.2.2.1]** Cross-Validation AUROC = 0.83569
|                      **[A.2.2.2]** Test AUROC = 0.85301
|      **[A.3]** **NB: Naive Bayes** (<mark style="background-color: #CCECFF">**klaR**</mark> package)
|             **[A.3.1]** **NB_FULL (Naive Bayes Without Recursive Feature Elimination)**   
|                      **[A.3.1.1]** Cross-Validation AUROC = 0.73904
|                      **[A.3.1.2]** Test AUROC = 0.68055
|             **[A.3.2]** **NB_RFE (Naive Bayes With Recursive Feature Elimination)**  
|                      **[A.3.2.1]** Cross-Validation AUROC = 0.76825
|                      **[A.3.2.2]** Test AUROC = 0.75231
|      **[A.4]** **LR: Logistic Regression** (<mark style="background-color: #CCECFF">**stats**</mark> package)
|             **[A.4.1]** **LR_FULL (Logistic Regression Without Recursive Feature Elimination)**  
|                      **[A.4.1.1]** Cross-Validation AUROC = 0.70733
|                      **[A.4.1.2]** Test AUROC = 0.77199
|             **[A.4.2]** **LR_RFE (Logistic Regression With Recursive Feature Elimination)**
|                      **[A.4.2.1]** Cross-Validation AUROC = 0.80296
|                      **[A.4.2.2]** Test AUROC = 0.86921
|      **[A.5]** **SVM_R: Support Vector Machine - Radial Basis Function Kernel** (<mark style="background-color: #CCECFF">**kernlab**</mark> package)
|             **[A.5.1]** **SVM_R_FULL (Support Vector Machine - Radial Basis Function Kernel Without Recursive Feature Elimination)** 
|                      **[A.5.1.1]** Cross-Validation AUROC = 0.85690
|                      **[A.5.1.2]** Test AUROC = 0.82060
|             **[A.5.2]** **SVM_R_RFE (Support Vector Machine - Radial Basis Function Kernel With Recursive Feature Elimination)** 
|                      **[A.5.2.1]** Cross-Validation AUROC = 0.84155
|                      **[A.5.2.2]** Test AUROC = 0.85995
|      **[A.6]** **KNN: K-Nearest Neighbors** (<mark style="background-color: #CCECFF">**caret**</mark> package)
|             **[A.6.1]** **KNN_FULL (K-Nearest Neighbors Without Recursive Feature Elimination)**
|                      **[A.6.1.1]** Cross-Validation AUROC = 0.79886
|                      **[A.6.1.2]** Test AUROC = 0.79167
|             **[A.6.2]** **KNN_RFE (K-Nearest Neighbors With Recursive Feature Elimination)**
|                      **[A.6.2.1]** Cross-Validation AUROC = 0.79215
|                      **[A.6.2.2]** Test AUROC = 0.84143
|
| **[B]** The models applied with recursive feature elimination which demonstrated the best and most consistent AUROC metrics are as follows:
|      **[B.1]** **SVM_R_RFE: Support Vector Machine - Radial Basis Function Kernel** (<mark style="background-color: #CCECFF">**kernlab**</mark> package)
|      **[B.2]** **LDA_RFE: Linear Discriminant Analysis** (<mark style="background-color: #CCECFF">**MASS**</mark> package)
|      **[B.3]** **RF_RFE: Random Forest** (<mark style="background-color: #CCECFF">**randomForest**</mark> package)
|
| **[C]** The most informative predictors consistently identified based from recursive feature elimination were as follows:
|      **[C.1]** <span style="color: #FF0000">MMP10</span> variable (numeric)
|      **[C.2]** <span style="color: #FF0000">Cystatin_C</span> variable (numeric)
|      **[C.3]** <span style="color: #FF0000">TRAIL_R3</span> variable (numeric)
|      **[C.4]** <span style="color: #FF0000">PAI_1</span> variable (numeric)
|      **[C.5]** <span style="color: #FF0000">GRO_alpha</span> variable (numeric)
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.6, warning=FALSE, message=FALSE}
##################################
# Consolidating all evaluation results
# for the train and test sets
# using the AUROC metric
##################################
Model <- c('RF_FULL','LDA_FULL','NB_FULL','LR_FULL','SVM_R_FULL','KNN_FULL','RF_RFE','LDA_RFE','NB_RFE','LR_RFE','SVM_R_RFE','KNN_RFE',
           'RF_FULL','LDA_FULL','NB_FULL','LR_FULL','SVM_R_FULL','KNN_FULL','RF_RFE','LDA_RFE','NB_RFE','LR_RFE','SVM_R_RFE','KNN_RFE')

Set <- c(rep('Cross-Validation',12),rep('Test',12))

ROCCurveAUC <- c(RF_FULL_Train_ROCCurveAUC,
                 LDA_FULL_Train_ROCCurveAUC,
                 NB_FULL_Train_ROCCurveAUC,
                 LR_FULL_Train_ROCCurveAUC,
                 SVM_R_FULL_Train_ROCCurveAUC,
                 KNN_FULL_Train_ROCCurveAUC,
                 RF_RFE_Train_ROCCurveAUC,
                 LDA_RFE_Train_ROCCurveAUC,
                 NB_RFE_Train_ROCCurveAUC,
                 LR_RFE_Train_ROCCurveAUC,
                 SVM_R_RFE_Train_ROCCurveAUC,
                 KNN_RFE_Train_ROCCurveAUC,
                 RF_FULL_Test_ROCCurveAUC,
                 LDA_FULL_Test_ROCCurveAUC,
                 NB_FULL_Test_ROCCurveAUC,
                 LR_FULL_Test_ROCCurveAUC,
                 SVM_R_FULL_Test_ROCCurveAUC,
                 KNN_FULL_Test_ROCCurveAUC,
                 RF_RFE_Test_ROCCurveAUC,
                 LDA_RFE_Test_ROCCurveAUC,
                 NB_RFE_Test_ROCCurveAUC,
                 LR_RFE_Test_ROCCurveAUC,
                 SVM_R_RFE_Test_ROCCurveAUC,
                 KNN_RFE_Test_ROCCurveAUC)

ROCCurveAUC_Summary <- as.data.frame(cbind(Model,Set,ROCCurveAUC))

ROCCurveAUC_Summary$ROCCurveAUC <- as.numeric(as.character(ROCCurveAUC_Summary$ROCCurveAUC))
ROCCurveAUC_Summary$Set <- factor(ROCCurveAUC_Summary$Set,
                                        levels = c("Cross-Validation",
                                                   "Test"))
ROCCurveAUC_Summary$Model <- factor(ROCCurveAUC_Summary$Model,
                                        levels = c('RF_FULL',
                                                   'RF_RFE',
                                                   'LDA_FULL',
                                                   'LDA_RFE',
                                                   'NB_FULL',
                                                   'NB_RFE',
                                                   'LR_FULL',
                                                   'LR_RFE',
                                                   'SVM_R_FULL',
                                                   'SVM_R_RFE',
                                                   'KNN_FULL',
                                                   'KNN_RFE'))

print(ROCCurveAUC_Summary, row.names=FALSE)

(ROCCurveAUC_Plot <- dotplot(Model ~ ROCCurveAUC,
                           data = ROCCurveAUC_Summary,
                           groups = Set,
                           main = "Classification Model Performance Comparison",
                           ylab = "Model",
                           xlab = "AUROC",
                           auto.key = list(adj=1, space="top", columns=2),
                           type=c("p", "h"),       
                           origin = 0,
                           alpha = 0.45,
                           pch = 16,
                           cex = 2))
```

</details>
|
# **2. Summary** <a name="summary"></a>
|
| ![](D:/Github_Codes/ProjectPortfolio/Portfolio_Project_17/images/Project17_Summary.png)
|
# **3. References**
|
| **[Book]** [Applied Predictive Modeling](http://appliedpredictivemodeling.com/) by Max Kuhn and Kjell Johnson
| **[Book]** [An Introduction to Statistical Learning](https://www.statlearning.com/) by Gareth James, Daniela Witten, Trevor Hastie and Rob Tibshirani
| **[Book]** [Multivariate Data Visualization with R](http://lmdvr.r-forge.r-project.org/figures/figures.html) by Deepayan Sarkar
| **[Book]** [Machine Learning](https://bookdown.org/ssjackson300/Machine-Learning-Lecture-Notes/) by Samuel Jackson
| **[Book]** [Data Modeling Methods](https://bookdown.org/larget_jacob/data-modeling-methods/) by Jacob Larget
| **[Book]** [Feature Engineering and Selection: A Practical Approach for Predictive Models](http://www.feat.engineering/index.html) by Max Kuhn and Kjell Johnson
| **[R Package]** [AppliedPredictiveModeling](https://cran.r-project.org/web//packages/AppliedPredictiveModeling/AppliedPredictiveModeling.pdf) by Max Kuhn
| **[R Package]** [caret](https://topepo.github.io/caret/index.html) by Max Kuhn
| **[R Package]** [rpart](https://mran.microsoft.com/web/packages/rpart/rpart.pdf) by Terry Therneau and Beth Atkinson
| **[R Package]** [lattice](https://cran.r-project.org/web/packages/lattice/lattice.pdf) by  Deepayan Sarkar
| **[R Package]** [dplyr](https://cran.r-project.org/web/packages/dplyr/index.html/) by Hadley Wickham
| **[R Package]** [moments](https://cran.r-project.org/web/packages/moments/index.html) by Lukasz Komsta and Frederick
| **[R Package]** [skimr](https://cran.r-project.org/web/packages/skimr/skimr.pdf) by  Elin Waring
| **[R Package]** [RANN](https://cran.r-project.org/web/packages/RANN/RANN.pdf) by  Sunil Arya, David Mount, Samuel Kemp and Gregory Jefferis
| **[R Package]** [corrplot](https://cran.r-project.org/web/packages/corrplot/corrplot.pdf) by Taiyun Wei
| **[R Package]** [tidyverse](https://cran.r-project.org/web/packages/tidyverse/tidyverse.pdf) by Hadley Wickham
| **[R Package]** [lares](https://cran.rstudio.com/web/packages/lares/lares.pdf) by Bernardo Lares
| **[R Package]** [DMwR2](https://mran.microsoft.com/snapshot/2016-05-02/web/packages/DMwR/DMwR.pdf) by Luis Torgo
| **[R Package]** [gridExtra](https://cran.r-project.org/web/packages/gridExtra/gridExtra.pdf) by Baptiste Auguie and Anton Antonov
| **[R Package]** [rattle](https://cran.r-project.org/web/packages/rattle/rattle.pdf) by Graham Williams
| **[R Package]** [rpart.plot](https://cran.r-project.org/web/packages/rpart.plot/rpart.plot.pdf) by Stephen Milborrow
| **[R Package]** [RColorBrewer](https://cran.r-project.org/web//packages/RColorBrewer/RColorBrewer.pdf) by Erich Neuwirth
| **[R Package]** [stats](https://search.r-project.org/R/refmans/stats/html/00Index.html) by R Core Team
| **[R Package]** [pls](https://cran.r-project.org/web/packages/pls/pls.pdf) by Kristian Hovde Liland
| **[R Package]** [nnet](https://cran.r-project.org/web/packages/nnet/nnet.pdf) by Brian Ripley
| **[R Package]** [elasticnet](https://cran.r-project.org/web/packages/elasticnet/elasticnet.pdf) by Hui Zou
| **[R Package]** [earth](https://cran.r-project.org/web/packages/earth/earth.pdf) by Stephen Milborrow
| **[R Package]** [party](https://cran.r-project.org/web/packages/party/party.pdf) by Torsten Hothorn
| **[R Package]** [kernlab](https://cran.r-project.org/web/packages/kernlab/kernlab.pdf) by Alexandros Karatzoglou
| **[R Package]** [randomForest](https://cran.r-project.org/web/packages/randomForest/randomForest.pdf) by Andy Liaw
| **[R Package]** [pROC](https://cran.r-project.org/web/packages/pROC/pROC.pdf) by Xavier Robin
| **[R Package]** [mda](https://cran.r-project.org/web/packages/mda/mda.pdf) by Trevor Hastie
| **[R Package]** [klaR](https://cran.r-project.org/web/packages/klaR/klaR.pdf) by Christian Roever, Nils Raabe, Karsten Luebke, Uwe Ligges, Gero Szepannek, Marc Zentgraf and David Meyer
| **[R Package]** [pamr](https://cran.r-project.org/web/packages/pamr/pamr.pdf) by Trevor Hastie, Rob Tibshirani, Balasubramanian Narasimhan and Gil Chu
| **[Article]** [The caret Package](https://topepo.github.io/caret/index.html) by Max Kuhn
| **[Article]** [A Short Introduction to the caret Package](https://cran.r-project.org/web/packages/caret/vignettes/caret.html) by Max Kuhn
| **[Article]** [Caret Package  A Practical Guide to Machine Learning in R](https://www.machinelearningplus.com/machine-learning/caret-package/#:~:text=Caret%20is%20short%20for%20Classification%20And%20REgression%20Training.,track%20of%20which%20algorithm%20resides%20in%20which%20package.) by Selva Prabhakaran
| **[Article]** [Tuning Machine Learning Models Using the Caret R Package](https://machinelearningmastery.com/tuning-machine-learning-models-using-the-caret-r-package/) by Jason Brownlee
| **[Article]** [Lattice Graphs](http://www.sthda.com/english/wiki/lattice-graphs) by Alboukadel Kassambara
| **[Article]** [A Tour of Machine Learning Algorithms](https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/) by Jason Brownlee
| **[Article]** [Recursive Feature Elimination: What It Is and Why It Matters](https://www.simplilearn.com/recursive-feature-elimination-article) by Karin Kelley
| **[Article]** [Recursive Feature Elimination (RFE) Example in Python](https://www.datatechnotes.com/2021/03/recursive-feature-elimination-rfe.html) by Otabek Yorkinov
| **[Article]** [Recursive Feature Elimination (RFE) for Feature Selection in Python](https://machinelearningmastery.com/rfe-feature-selection-in-python/) by Jason Brownlee
| **[Article]** [Getting started with Recursive Feature Elimination algorithm in Machine Learning](https://www.section.io/engineering-education/recursive-feature-elimination/) by Daniel Mwanthi
| **[Article]** [Guide To Dimensionality Reduction With Recursive Feature Elimination](https://analyticsindiamag.com/guide-to-dimensionality-reduction-with-recursive-feature-elimination/) by Vijaysinh Lendave
| **[Article]** [Decision Tree Algorithm Examples In Data Mining](https://www.softwaretestinghelp.com/decision-tree-algorithm-examples-data-mining/) by Software Testing Help Team
| **[Article]** [4 Types of Classification Tasks in Machine Learning](https://machinelearningmastery.com/types-of-classification-in-machine-learning/) by Jason Brownlee
| **[Article]** [Spot-Check Classification Machine Learning Algorithms in Python with scikit-learn](https://machinelearningmastery.com/spot-check-classification-machine-learning-algorithms-python-scikit-learn/) by Jason Brownlee
| **[Article]** [An Introduction to Naive Bayes Algorithm for Beginners](https://www.turing.com/kb/an-introduction-to-naive-bayes-algorithm-for-beginners) by Turing Team
| **[Article]** [How Naive Bayes Algorithm Works?](https://www.machinelearningplus.com/predictive-modeling/how-naive-bayes-algorithm-works-with-example-and-full-code/#8buildinganaivebayesclassifierinr) by Selva Prabhakaran
| **[Article]** [Naive Bayes Classifiers](https://www.geeksforgeeks.org/naive-bayes-classifiers/) by Geeks For Geeks Team
| **[Article]** [Machine Learning Tutorial: A Step-by-Step Guide for Beginners](http://www.feat.engineering/index.html) by Mayank Banoula
| **[Article]** [Discriminant Analysis Essentials in R](http://www.sthda.com/english/articles/36-classification-methods-essentials/146-discriminant-analysis-essentials-in-r/) by Alboukadel Kassambara
| **[Article]** [Linear Discriminant Analysis, Explained](https://yangxiaozhou.github.io/data/2019/10/02/linear-discriminant-analysis.html) by Xiaozhou Yang
| **[Article]** [Classification Tree](https://support.bccvl.org.au/support/solutions/articles/6000083204-classification-tree) by BCCVL Team
| **[Article]** [K-Nearest Neighbors (KNN) Classification with scikit-learn](https://www.datacamp.com/tutorial/k-nearest-neighbor-classification-scikit-learn) by Datacamp Team
| **[Article]** [Mathematical explanation of K-Nearest Neighbour](https://www.geeksforgeeks.org/mathematical-explanation-of-k-nearest-neighbour/) by Geeks for Geeks Team
| **[Article]** [What Is The K-Nearest Neighbors Algorithm?](https://www.ibm.com/topics/knn) by IBM Team
| **[Article]** [Random Forest](https://support.bccvl.org.au/support/solutions/articles/6000083217-random-forest) by BCCVL Team
| **[Article]** [Generalized Linear Model](https://support.bccvl.org.au/support/solutions/articles/6000083213-generalized-linear-model) by BCCVL Team
| **[Article]** [Introduction to Support Vector Machines](https://www.geeksforgeeks.org/introduction-to-support-vector-machines-svm/) by Geeks for Geeks Team
| **[Article]** [Support Vector Machines: A Simple Explanation](https://www.kdnuggets.com/2016/07/support-vector-machines-simple-explanation.html) by Noel Bambrick
| **[Publication]** [Enhanced Recursive Feature Elimination](https://ieeexplore.ieee.org/document/4457268) by Xuewen Chen and Jong Cheol Jeong (International Conference on Machine Learning and Applications (ICMLA))
| **[Publication]** [Bagging Predictors ](https://link.springer.com/content/pdf/10.1007/BF00058655.pdf) by Leo Breiman (Machine Learning)
| **[Publication]** [The Use of Multiple Measurements in Taxonomic Problems](https://www.semanticscholar.org/paper/THE-USE-OF-MULTIPLE-MEASUREMENTS-IN-TAXONOMIC-Fisher/ab21376e43ac90a4eafd14f0f02a0c87502b6bbf) by Ronald Fisher (Annals of Human Genetics)
| **[Publication]** [The Origins of Logistic Regression](http://dx.doi.org/10.2139/ssrn.360300) by JS Cramer (Econometrics eJournal)
| **[Publication]** [Who Discovered Bayes's Theorem?](https://www.jstor.org/stable/2682766) by Stephen Stigler (The American Statistician)
| **[Publication]** [Nearest Neighbor Pattern Classification](https://isl.stanford.edu/~cover/papers/transIT/0021cove.pdf) by T Cover and P Hart (Transactions of Information Theory)
| **[Publication]** [Support Vector Regression Machines](http://www.cs.cmu.edu/~pakyan/compbio/references/Drucker_NIPS_1996.pdf) by Harris Drucker, Chris Burges, Linda Kaufman, Alex Smola and Vladimir Vapnik (Advances in Neural Information Processing Systems)
| **[Course]** [Applied Data Mining and Statistical Learning](https://online.stat.psu.edu/stat508/) by Penn State Eberly College of Science
| **[Course]** [Regression Methods](https://online.stat.psu.edu/stat501/) by Penn State Eberly College of Science
| **[Course]** [Applied Regression Analysis](https://online.stat.psu.edu/stat462/) by Penn State Eberly College of Science
| **[Course]** [Applied Data Mining and Statistical Learning](https://online.stat.psu.edu/stat508/) by Penn State Eberly College of Science
|
|
|
|